{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSF 592 Project: Phase II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a 10 x 10 matrix of city limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data for city boundaries\n",
    "data_city_boundary = pd.read_csv('City_Boundary_layer.csv')\n",
    "\n",
    "# Manipulate polygon string to get list of coordinates\n",
    "polygon = data_city_boundary['the_geom'].values[0]\n",
    "polygon = polygon[10:-2]\n",
    "coordinates_city_limits = polygon.split(', ')\n",
    "\n",
    "# Get a list of longitudes and latitudes for the city limits\n",
    "longitudes_city_limits = []\n",
    "latitudes_city_limits = []\n",
    "for long_lat in coordinates_city_limits:\n",
    "    long, lat = long_lat.split()\n",
    "    long = float(long)\n",
    "    lat = float(lat)\n",
    "    longitudes_city_limits.append(long)\n",
    "    latitudes_city_limits.append(lat)\n",
    "    \n",
    "# Find absolute city limits as the min/max longitude/latitude\n",
    "overall_min_long = min(longitudes_city_limits)\n",
    "overall_max_long = max(longitudes_city_limits)\n",
    "overall_min_lat = min(latitudes_city_limits)\n",
    "overall_max_lat = max(latitudes_city_limits)\n",
    "\n",
    "# Split the city into a 10 x 10 matrix\n",
    "# Create a 10x10 matrix\n",
    "quadrantdf = pd.DataFrame(columns = ('min_lat', 'max_lat', 'min_long', 'max_long'))\n",
    "quadrant = [[0 for x in range(10)] for y in range(10)]\n",
    "\n",
    "# Populate quadrant boundaries\n",
    "for i in range(0,10):\n",
    "    # Get the min/max longitude for the quadrant\n",
    "    min_long = overall_min_long + (overall_max_long - overall_min_long) / 10 * i\n",
    "    max_long = overall_min_long + (overall_max_long - overall_min_long) / 10 * (i + 1)\n",
    "    for j in range(0, 10):\n",
    "        # Get the min/max latitude for the quadrant\n",
    "        min_lat = overall_min_lat + (overall_max_lat - overall_min_lat) / 10 * j\n",
    "        max_lat = overall_min_lat + (overall_max_lat - overall_min_lat) / 10 * (j + 1)\n",
    "        # Populate data into a dataframe for each quadrant\n",
    "        quadrant[j][i] = {'min_lat':min_lat,'max_lat':max_lat,'min_long':min_long,'max_long':max_long}\n",
    "        for k in range (0, 100):\n",
    "            quadrantdf.loc[k] = [min_lat, max_lat, min_long, max_long] #each row represents a grids coordinates\n",
    "df = pd.DataFrame(quadrant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a map of the city limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start maps at the center of Calgary\n",
    "map_overall_boundaries = folium.Map(location=[51.0447, -114.0719], zoom_start=10)\n",
    "map_quadrant_boundaries = folium.Map(location=[51.0447, -114.0719], zoom_start=10)\n",
    "\n",
    "# Place rectangles\n",
    "# Place rectangle for the overall map\n",
    "folium.vector_layers.Rectangle([(overall_max_lat, overall_max_long), \\\n",
    "                                (overall_max_lat, overall_min_long), \\\n",
    "                                (overall_min_lat, overall_max_long), \\\n",
    "                                (overall_min_lat, overall_min_long)]).add_to(map_overall_boundaries)\n",
    "            \n",
    "# Place rectangle for each quadrant\n",
    "for i in range(0,10):\n",
    "    for j in range(0, 10):\n",
    "        folium.vector_layers.Rectangle([(quadrant[i][j]['max_lat'], quadrant[i][j]['max_long']), \\\n",
    "                                        (quadrant[i][j]['max_lat'], quadrant[i][j]['min_long']), \\\n",
    "                                        (quadrant[i][j]['min_lat'], quadrant[i][j]['max_long']), \\\n",
    "                                        (quadrant[i][j]['min_lat'], quadrant[i][j]['min_long'])]).add_to(map_quadrant_boundaries)\n",
    "\n",
    "# Save maps\n",
    "map_overall_boundaries.save('map_overall_boundaries.html')\n",
    "map_quadrant_boundaries.save('map_quadrant_boundaries.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file\n",
    "data_speed_limit = pd.read_csv('Speed_Limits.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "data_speed_limit.drop(columns=['BOUND', 'CREATED_DT'], inplace=True)\n",
    "\n",
    "# Remove \"MULTILINESTRING(........)\"\n",
    "data_speed_limit['multiline'] = data_speed_limit['multiline'].str.slice(17, -1)\n",
    "\n",
    "for index, row in data_speed_limit.iterrows():\n",
    "    # Get multiline string\n",
    "    multi_line = row['multiline']\n",
    "    \n",
    "    # Split multiline into lines\n",
    "    lines = multi_line.split(')')\n",
    "    \n",
    "    coordinates = []\n",
    "    for line in lines:\n",
    "        # Remove brackets\n",
    "        line = line.replace(')','')\n",
    "        line = line.replace('(','')\n",
    "        \n",
    "        # Get all latitude and longitude\n",
    "        long_lat_array = line.strip().split(',')\n",
    "        line_coordinates = []\n",
    "        for long_lat in long_lat_array:\n",
    "            # Get latitude and longitude\n",
    "            coordinate = long_lat.strip().split(' ')\n",
    "            \n",
    "            # If you have an empty list then do nothing\n",
    "            if coordinate[0] == \"\":\n",
    "                continue\n",
    "            \n",
    "            coordinate[0] = float(coordinate[0])\n",
    "            coordinate[1] = float(coordinate[1])\n",
    "\n",
    "            # Append to lists\n",
    "            line_coordinates.append([coordinate[1], coordinate[0]])\n",
    "        if not line_coordinates:\n",
    "            continue\n",
    "        coordinates.append(line_coordinates)\n",
    "    \n",
    "    # Update dataframe\n",
    "    data_speed_limit.at[index,'multiline'] = coordinates\n",
    "\n",
    "# Rename multiline column\n",
    "data_speed_limit.rename(columns={\"multiline\": \"coordinates\"}, inplace=True)\n",
    "\n",
    "# Calculate lines\n",
    "lat_avg = []\n",
    "long_avg = []\n",
    "speed = []\n",
    "distance = []\n",
    "# Loop through every road segment\n",
    "for index, row in data_speed_limit.iterrows():\n",
    "    coordinates = row['coordinates']\n",
    "    # Loop through every line in a road segment\n",
    "    for multipoint_line in coordinates:\n",
    "        # Loop through every point\n",
    "        max_lim = len(multipoint_line)-1\n",
    "        for i in range(max_lim):\n",
    "            lat_avg.append((multipoint_line[i][0] + multipoint_line[i+1][0])/2)\n",
    "            long_avg.append((multipoint_line[i][1] + multipoint_line[i+1][1])/2)\n",
    "            speed.append(row['SPEED'])\n",
    "            distance.append(((multipoint_line[i][0] - multipoint_line[i+1][0])**2 \\\n",
    "                                  + (multipoint_line[i][1] - multipoint_line[i+1][1])**2)**0.5)\n",
    "    \n",
    "data_speed_limit_analysed = pd.DataFrame({'lat_avg':lat_avg, \\\n",
    "                                          'long_avg':long_avg, \\\n",
    "                                          'speed':speed, \\\n",
    "                                          'distance':distance})\n",
    "data_speed_limit_analysed['weighted_speed'] = data_speed_limit_analysed['distance'] * data_speed_limit_analysed['speed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file\n",
    "data_traffic_volume = pd.read_csv('Traffic_Volumes_for_2018.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "data_traffic_volume.drop(columns=['YEAR'], inplace=True)\n",
    "\n",
    "# Remove \"MULTILINESTRING(........)\"\n",
    "data_traffic_volume['multilinestring'] = data_traffic_volume['multilinestring'].str.slice(17, -1)\n",
    "\n",
    "for index, row in data_traffic_volume.iterrows():\n",
    "    # Get multiline string\n",
    "    multi_line = row['multilinestring']\n",
    "    \n",
    "    # Split multiline into lines\n",
    "    lines = multi_line.split(')')\n",
    "    \n",
    "    coordinates = []\n",
    "    for line in lines:\n",
    "        # Remove brackets\n",
    "        line = line.replace(')','')\n",
    "        line = line.replace('(','')\n",
    "        \n",
    "        # Get all latitude and longitude\n",
    "        long_lat_array = line.strip().split(',')\n",
    "        line_coordinates = []\n",
    "        for long_lat in long_lat_array:\n",
    "            # Get latitude and longitude\n",
    "            coordinate = long_lat.strip().split(' ')\n",
    "            \n",
    "            # If you have an empty list then do nothing\n",
    "            if coordinate[0] == \"\":\n",
    "                continue\n",
    "            \n",
    "            coordinate[0] = float(coordinate[0])\n",
    "            coordinate[1] = float(coordinate[1])\n",
    "\n",
    "            # Append to lists\n",
    "            line_coordinates.append([coordinate[1], coordinate[0]])\n",
    "        if not line_coordinates:\n",
    "            continue\n",
    "        coordinates.append(line_coordinates)\n",
    "    \n",
    "    # Update dataframe\n",
    "    data_traffic_volume.at[index,'multilinestring'] = coordinates\n",
    "\n",
    "# Rename multiline column\n",
    "data_traffic_volume.rename(columns={\"multilinestring\": \"coordinates\"}, inplace=True)\n",
    "\n",
    "# Calculate lines\n",
    "lat_avg = []\n",
    "long_avg = []\n",
    "volume = []\n",
    "distance = []\n",
    "sec_name = []\n",
    "# Loop through every road segment\n",
    "for index, row in data_traffic_volume.iterrows():\n",
    "    coordinates = row['coordinates']\n",
    "    # Loop through every line in a road segment\n",
    "    for multipoint_line in coordinates:\n",
    "        # Loop through every point\n",
    "        max_lim = len(multipoint_line)-1\n",
    "        for i in range(max_lim):\n",
    "            lat_avg.append((multipoint_line[i][0] + multipoint_line[i+1][0])/2)\n",
    "            long_avg.append((multipoint_line[i][1] + multipoint_line[i+1][1])/2)\n",
    "            volume.append(row['VOLUME'])\n",
    "            distance.append(((multipoint_line[i][0] - multipoint_line[i+1][0])**2 \\\n",
    "                                  + (multipoint_line[i][1] - multipoint_line[i+1][1])**2)**0.5)\n",
    "            sec_name.append(row['SECNAME'])\n",
    "    \n",
    "data_traffic_volume_analysed = pd.DataFrame({'lat_avg':lat_avg, \\\n",
    "                                              'long_avg':long_avg, \\\n",
    "                                              'volume':volume, \\\n",
    "                                              'distance':distance, \\\n",
    "                                              'sec_name':sec_name})\n",
    "\n",
    "# Calculate the total length of each road\n",
    "road_lengths = data_traffic_volume_analysed.groupby('sec_name')['distance'].sum().to_dict()\n",
    "data_traffic_volume_analysed['total_road_length'] = data_traffic_volume_analysed['sec_name'].apply(lambda x: road_lengths[x])\n",
    "\n",
    "# Calculate the weighted volume\n",
    "data_traffic_volume_analysed['volume'] *= data_traffic_volume_analysed['distance'] / data_traffic_volume_analysed['total_road_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic Cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file\n",
    "data_traffic_cameras = pd.read_csv('Traffic_Camera_Locations.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "data_traffic_cameras.drop(columns=['Camera Location', 'Quadrant', 'Camera URL'], inplace=True)\n",
    "\n",
    "# Drop any NaN values\n",
    "data_traffic_cameras.dropna(inplace=True)\n",
    "\n",
    "# Add a count column\n",
    "data_traffic_cameras['Count'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file\n",
    "data_traffic_signals = pd.read_csv('Traffic_Signals.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "data_traffic_signals.drop(columns=['INSTDATE', 'FIRSTROAD', 'SECONDROAD', 'QUADRANT',\n",
    "                                   'INT_TYPE', 'PEDBUTTONS', 'PED_TIMER', \n",
    "                                   'ACCESSIBLE PEDESTRIAN SIGNAL', 'Point'], inplace=True)\n",
    "\n",
    "# Drop any NaN values\n",
    "data_traffic_signals.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file\n",
    "data_traffic_signs = pd.read_csv('Traffic_Signs.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "data_traffic_signs.drop(columns=['BLADE_TYPE', 'COMMENTS', 'FACING_CD', 'FLD_SRC_TXT', \n",
    "                                 'INSTDATE', 'MATERIAL', 'PL_TY', 'REUSE', 'SGN_STA_CD',\n",
    "                                 'SIGN_TXT', 'STA_CD', 'SIZE_CD', 'SUPPORTTYPE', 'UNITID',\n",
    "                                 'TE_SIGNLOCATION_UNITID', 'Ward Boundaries', 'City Quadrants',\n",
    "                                 'Calgary Communities', 'Ward Boundaries 2013-2017'], inplace=True)\n",
    "\n",
    "# Drop any NaN values\n",
    "data_traffic_signs.dropna(inplace=True)\n",
    "\n",
    "# Get long and lat from point\n",
    "data_traffic_signs['longitude'] = data_traffic_signs['POINT']\n",
    "data_traffic_signs['latitude'] = data_traffic_signs['POINT']\n",
    "data_traffic_signs['longitude'] = data_traffic_signs['longitude'].apply(lambda x: float(x[7:x.index(' ', 7)]))\n",
    "data_traffic_signs['latitude'] = data_traffic_signs['latitude'].apply(lambda x: float(x[x.index(' ', 7):-1]))\n",
    "\n",
    "# Drop point column\n",
    "data_traffic_signs.drop(columns=['POINT'], inplace=True)\n",
    "\n",
    "# Rename count column to 'count'\n",
    "data_traffic_signs.rename(columns={\"SGN_COUNT_NO\": \"Count\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Date/Time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\ENSF592\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date/Time'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4a392099c526>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mweather_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_weather_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50430\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweather_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date/Time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ENSF592\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ENSF592\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date/Time'"
     ]
    }
   ],
   "source": [
    "# Need a dataframe with lat, long, avg temp, and avg visibility at the location\n",
    "quadrantdf['average temperature'] = 'Nan'\n",
    "quadrantdf['average visibility'] = 'Nan'\n",
    "\n",
    "def download_weather_data(station,year,month,daily=True):\n",
    "    if(month == 13):\n",
    "        return;\n",
    "    url_template = \"https://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&stationID={0}&Year={1}&Month={2}&Day=14&timeframe={3}&submit=Download+Data\"\n",
    "    url = url_template.format(station,year,month,1)\n",
    "    weather_data = pd.read_csv(url, index_col='Date/Time',parse_dates=True)\n",
    "    weather_data.columns = [col.replace('\\xb0','') for col in weather_data.columns]\n",
    "    weather_data = weather_data.append(download_weather_data(station,year,month+1))\n",
    "    return weather_data\n",
    "\n",
    "weather_data = download_weather_data(50430,2018,1)\n",
    "\n",
    "for row in weather_data['Date/Time']:\n",
    "    str.split(\" \")\n",
    "    \n",
    "weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file\n",
    "data_traffic_accidents = pd.read_csv('Traffic_Incidents.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "data_traffic_accidents.drop(columns=['DESCRIPTION', 'MODIFIED_DT', 'QUADRANT', 'location', 'id'], inplace=True)\n",
    "\n",
    "# Drop any NaN values\n",
    "data_traffic_accidents.dropna(inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "data_traffic_accidents.rename(columns={\"Latitude\": \"latitude\", \"Longitude\":\"longitude\"}, inplace=True)\n",
    "\n",
    "#for row in data_traffic_accidents['START_DT']:\n",
    "#    data_traffic_accidents[\"START_DT\"]= data_traffic_accidents[\"START_DT\"].str.split(\" \", n = 1, expand = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data to quadrant dictionary \n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        # Calculate average speed\n",
    "        speed_d = data_speed_limit_analysed[((data_speed_limit_analysed['lat_avg'] >= quadrant[i][j]['min_lat']) &\n",
    "                                            (data_speed_limit_analysed['lat_avg'] < quadrant[i][j]['max_lat']) &\n",
    "                                            (data_speed_limit_analysed['long_avg'] >= quadrant[i][j]['min_long']) &\n",
    "                                            (data_speed_limit_analysed['long_avg'] < quadrant[i][j]['max_long']))]['distance'].sum()\n",
    "        speed_s = data_speed_limit_analysed[((data_speed_limit_analysed['lat_avg'] >= quadrant[i][j]['min_lat']) &\n",
    "                                            (data_speed_limit_analysed['lat_avg'] < quadrant[i][j]['max_lat']) &\n",
    "                                            (data_speed_limit_analysed['long_avg'] >= quadrant[i][j]['min_long']) &\n",
    "                                            (data_speed_limit_analysed['long_avg'] < quadrant[i][j]['max_long']))]['weighted_speed'].sum()\n",
    "        if speed_d == 0:\n",
    "            quadrant[i][j]['Average Speed'] = 0\n",
    "        else:\n",
    "            quadrant[i][j]['Average Speed'] = speed_s/ speed_d\n",
    "        \n",
    "        # Calculate average traffic volume\n",
    "        quadrant[i][j]['Average Traffic Volume'] = data_traffic_volume_analysed[((data_traffic_volume_analysed['lat_avg'] >= quadrant[i][j]['min_lat']) &\n",
    "                                                                            (data_traffic_volume_analysed['lat_avg'] < quadrant[i][j]['max_lat']) &\n",
    "                                                                            (data_traffic_volume_analysed['long_avg'] >= quadrant[i][j]['min_long']) &\n",
    "                                                                            (data_traffic_volume_analysed['long_avg'] < quadrant[i][j]['max_long']))]['volume'].sum()\n",
    "\n",
    "        \n",
    "        # Calculate number of cameras, signals, and signs\n",
    "        quadrant[i][j]['Number of Traffic Cameras'] = data_traffic_cameras[((data_traffic_cameras['latitude'] >= quadrant[i][j]['min_lat']) &\n",
    "                                                                            (data_traffic_cameras['latitude'] < quadrant[i][j]['max_lat']) &\n",
    "                                                                            (data_traffic_cameras['longitude'] >= quadrant[i][j]['min_long']) &\n",
    "                                                                            (data_traffic_cameras['longitude'] < quadrant[i][j]['max_long']))]['Count'].sum()\n",
    "        quadrant[i][j]['Number of Traffic Signals'] = data_traffic_signals[((data_traffic_signals['latitude'] >= quadrant[i][j]['min_lat']) &\n",
    "                                                                            (data_traffic_signals['latitude'] < quadrant[i][j]['max_lat']) &\n",
    "                                                                            (data_traffic_signals['longitude'] >= quadrant[i][j]['min_long']) &\n",
    "                                                                            (data_traffic_signals['longitude'] < quadrant[i][j]['max_long']))]['Count'].sum()\n",
    "        quadrant[i][j]['Number of Traffic Signs'] = data_traffic_signs[((data_traffic_signs['latitude'] >= quadrant[i][j]['min_lat']) &\n",
    "                                                                        (data_traffic_signs['latitude'] < quadrant[i][j]['max_lat']) &\n",
    "                                                                        (data_traffic_signs['longitude'] >= quadrant[i][j]['min_long']) &\n",
    "                                                                        (data_traffic_signs['longitude'] < quadrant[i][j]['max_long']))]['Count'].sum()\n",
    "        \n",
    "        # Calculate weather conditions\n",
    "        quadrant[i][j]['Average Temperature'] = 0\n",
    "        quadrant[i][j]['Average Visibility'] = 0\n",
    "        \n",
    "        # Calculate number of accidents\n",
    "        quadrant[i][j]['Number of Traffic Accidents'] = data_traffic_accidents[((data_traffic_accidents['latitude'] >= quadrant[i][j]['min_lat']) &\n",
    "                                                                                (data_traffic_accidents['latitude'] < quadrant[i][j]['max_lat']) &\n",
    "                                                                                (data_traffic_accidents['longitude'] >= quadrant[i][j]['min_long']) &\n",
    "                                                                                (data_traffic_accidents['longitude'] < quadrant[i][j]['max_long']))]['Count'].sum()\n",
    "        \n",
    "# Pass quadrant data into a dataframe\n",
    "quadrant_df = pd.DataFrame(index=range(100),columns=['min_lat', 'max_lat','min_long', 'max_long', \\\n",
    "                                                     'Average Speed', 'Average Traffic Volume', 'Number of Traffic Cameras', \\\n",
    "                                                     'Number of Traffic Signals', 'Number of Traffic Signs', \\\n",
    "                                                     'Average Temperature', 'Average Visibility', 'Number of Traffic Accidents'])\n",
    "k = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        quadrant_df.iloc[k] = quadrant[i][j]\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay signal locations on a heatmap\n",
    "\n",
    "# Heatmap of Accidents:\n",
    "acc_heatmap = folium.Map(location=[51.0447, -114.0719], zoom_start=10)\n",
    "acc_data = [[row['Latitude'],row['Longitude']] for index, row in data_traffic_accidents.iterrows()]\n",
    "HeatMap(acc_data).add_to(acc_heatmap)\n",
    "acc_heatmap.save('Full_Accident_Heatmap.html')\n",
    "\n",
    "# Add traffic signal markers\n",
    "signal_cds = [[row['latitude'],row['longitude']] for index, row in data_traffic_signals.iterrows()]\n",
    "for cds in range(len(signal_cds)):\n",
    "    folium.Marker(signal_cds[cds]).add_to(acc_heatmap)\n",
    "acc_heatmap.save('Full_Accident_Heatmap(signals).html')\n",
    "\n",
    "# Hide accidents that appear within 100m of a signal\n",
    "no_signal_acc = []\n",
    "for i in range(len(data_traffic_accidents)):\n",
    "    # Generate location of the current accident\n",
    "    accident_loc = [data_traffic_accidents['Latitude'][i],data_traffic_accidents['Longitude'][i]]\n",
    "    plot = True\n",
    "    # Check the accident location against all signal locations\n",
    "    for cds in range(len(signal_cds)):\n",
    "        # Find the distance between the accident and the current signal location\n",
    "        lon_dif = accident_loc[1]-signal_cds[cds][1]\n",
    "        lat_dif = accident_loc[0]-signal_cds[cds][0]\n",
    "        lon_dif_meters = lon_dif*69000\n",
    "        lat_dif_meters = lat_dif*111000\n",
    "        distance = (lon_dif_meters**2+lat_dif_meters**2)**0.5\n",
    "        # Accident is within 100m of a signal, break and check the next signal\n",
    "        if distance<100:\n",
    "            plot = False\n",
    "            break\n",
    "    # Accident was not found to be within range of a signal, \n",
    "    # add it to the list of accidents to plot\n",
    "    if plot==True:\n",
    "        no_signal_acc.append(accident_loc)\n",
    "        \n",
    "# Generate map for the accidents that occur outside of a signal\n",
    "no_signal_map = folium.Map(location = [51.0447, -114.0719], zoom_start = 10)\n",
    "HeatMap(no_signal_acc).add_to(no_signal_map)\n",
    "no_signal_map.save('Filtered_Accident_HM.html')\n",
    "\n",
    "signal_cds = [[row['latitude'],row['longitude']] for index, row in data_traffic_signals.iterrows()]\n",
    "signal_map = no_signal_map\n",
    "for cds in range(len(signal_cds)):\n",
    "    folium.Marker(signal_cds[cds]).add_to(signal_map)\n",
    "signal_map.save('Filtered_Accident_HM(signals).html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Speed Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the map\n",
    "map_speed_limits = folium.Map(location=[51.0447, -114.0719], zoom_start=10)\n",
    "\n",
    "# Go through every road\n",
    "for index, row in data_speed_limit.iterrows():\n",
    "    # Get coordinates for the road\n",
    "    coordinates = row['coordinates']\n",
    "    for line in coordinates:\n",
    "        # Don't do anything if list is empty\n",
    "        if not line:\n",
    "            continue\n",
    "        # Get color intensity based on speed limit\n",
    "        color = '#%02x%02x%02x' % (200, int(max((110-row['SPEED']) * 255 / 110, 0)), int(max((110-row['SPEED']) * 255 / 110, 0)))\n",
    "        # Add line to the map\n",
    "        folium.PolyLine(locations=line,weight=3, color=color).add_to(map_speed_limits)\n",
    "        \n",
    "\n",
    "# Save map\n",
    "map_speed_limits.save('map_speed_limits.html')\n",
    "map_speed_limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Traffic Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a heat map of the traffic volume"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
